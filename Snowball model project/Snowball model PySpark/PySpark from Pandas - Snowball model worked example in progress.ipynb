{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ef0e77",
   "metadata": {},
   "source": [
    "# Practical guide for translating Snowball model from Pandas to PySpark\n",
    "\n",
    "The aim of this guide is to provide a worked example for translating Pandas operations on DataFrames to PySpark, and highlighting any difficulties or benefits in doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab361e",
   "metadata": {},
   "source": [
    "## Setting up SparkSession in PySpark\n",
    "\n",
    "To begin, we will go through the steps of setting up a so-called _SparkSession_, which provides us with an entry point for working with Spark DataFrame and connecting to (Apache) Spark, the underlying analytics engine of PySpark. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance, allowing users to  execute large-scale data processing.\n",
    "\n",
    "More infomation about connecting to Spark using SparkSession can be found here https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html.\n",
    "\n",
    "Databricks Notebook automatically sets up a SparkSession, which can be accessed using the variable named 'spark'.\n",
    "\n",
    "To run PySpark code locally you must set up a SparkSession manually, which can be done by simply running the following three lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123d14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[1]').appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b9909",
   "metadata": {},
   "source": [
    "## Importing .csv file as a Spark DataFrame\n",
    "\n",
    "There are many ways to build a Spark DataFrame, but for our purposes we do so by importing from a .csv file. In order to make comparisions between Pandas and PySpark operations we also import the same dataset as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8ffadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# record path for example revenue data\n",
    "path = '..\\Snowball model Pandas\\Alteryx datasets\\Snowball example inputs v2.csv'\n",
    "\n",
    "# import revenue data as a Pandas DataFrame\n",
    "revenue_data_pd = pd.read_csv(path)\n",
    "\n",
    "# import .csv file as PySpark DataFrame\n",
    "revenue_data_ps = spark.read.csv(path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17bdc5d",
   "metadata": {},
   "source": [
    "## Translating operations in clean_date_columns        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b331e8",
   "metadata": {},
   "source": [
    "Below we define the function clean_date_columns, which was used in our Python Snowball model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2940939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_date_columns_pd(dataset, date_columns=['Month']):\n",
    "    \"\"\"\n",
    "            Returns revenue data with timestamps in revenue_data[date_column] set as datetime objects and normalised to\n",
    "            have day=1.\n",
    "                Args:\n",
    "                    revenue_data (pandas.DataFrame): list of payments.\n",
    "                    date_columns (str): titles for columns with dates, entries of which are currently datetime objects\n",
    "                                        or standard format datetime strings.\n",
    "                Returns:\n",
    "                    revenue_data_clean (pandas.DataFrame): list of payments.\n",
    "            \"\"\"\n",
    "    for date_column in date_columns:\n",
    "        # cast entries in date_column from string to datetime in case they are not already\n",
    "        dataset.loc[:, date_column] = pd.to_datetime(dataset[date_column])\n",
    "\n",
    "        # normalize dates of all payments to be the first day of the given month\n",
    "        dataset.loc[:, date_column] = dataset[date_column].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "    dataset_clean = dataset\n",
    "\n",
    "    return dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5291b06",
   "metadata": {},
   "source": [
    "Given a dataset with one or more columns of dates, the two operations carried out by the clean_date_columns function written using Pandas are as follows\n",
    "- Cast datatype of entries to date columns as Python datetime objects.\n",
    "- Normalize all dates appearing in these date columns to the first day of each given month.\n",
    "\n",
    "These operations can be replicated in PySpark using Date manipulation tools from the pyspark.sql.functions module applied to PySpark SQL DateType objects. In the cell below we define a PySpark version of clean_date_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7af2eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trunc, to_date\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "def clean_date_columns_ps(dataset, date_columns=['Month']):\n",
    "    \"\"\"\n",
    "        Returns revenue data with timestamps in revenue_data[date_column] set as datetime objects and normalised to\n",
    "        have the first day of each month.\n",
    "            Args:\n",
    "                dataset (Spark DataFrame): dataset with some date columns.\n",
    "                date_columns (str): titles for columns with payment dates currently datetime objects or standard\n",
    "                                    format datetime strings.\n",
    "            Returns:\n",
    "                revenue_data_clean (Spark DataFrame): dateset with date columns set to datetime objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast datatype of entries to date columns from String to PySpark DateType objects\n",
    "    dataset_dt = dataset.select(*(to_date(col(c)).alias(c) for c in date_columns))\n",
    "\n",
    "    # normalize dates of all payments to be the first day of the given month\n",
    "    dataset_clean = dataset_dt.select(*(trunc(col(c), \"Month\").alias(c) for c in date_columns))\n",
    "\n",
    "    return dataset_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e83be1",
   "metadata": {},
   "source": [
    "In the cells below we apply both clean_date_columns functions to the respective Pandas and PySpark DataFrames, and check that the desired changes have been made to the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d88f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially, the date column in our Pandas DataFrame has dtype given as follows\n",
      "Month    object\n",
      "dtype: object\n",
      "Here is a small sample of entries:\n",
      "        Month\n",
      "0  2019-04-01\n",
      "1  2019-12-01\n",
      "2  2019-04-01\n",
      "3  2019-12-01\n",
      "4  2019-12-01\n",
      "Initially, the date column in our Pandas DataFrame has datatype given as follows\n",
      "root\n",
      " |-- Month: string (nullable = true)\n",
      "\n",
      "Here is a small sample of entries:\n",
      "+----------+\n",
      "|     Month|\n",
      "+----------+\n",
      "|2019-04-01|\n",
      "|2019-12-01|\n",
      "|2019-04-01|\n",
      "|2019-12-01|\n",
      "|2019-12-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check datatype and content of date column in Pandas DataFrame before applying clean_date_columns_pd \n",
    "print(\"Initially, the date column in our Pandas DataFrame has dtype given as follows\")\n",
    "print(revenue_data_pd[date_columns].dtypes)\n",
    "print(\"Here is a small sample of entries:\")\n",
    "print(revenue_data_pd[date_columns].head(5))\n",
    "\n",
    "# check datatype and content of date column in PySpark DataFrame before applying clean_date_columns_ps\n",
    "print(\"Initially, the date column in our PySpark DataFrame has datatype given as follows\")\n",
    "revenue_data_ps.select(*(col(c) for c in date_columns)).printSchema()\n",
    "print(\"Here is a small sample of entries:\")\n",
    "print(revenue_data_ps[date_columns].show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e3dfce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of date column titles\n",
    "date_columns = ['Month']\n",
    "\n",
    "# clean date column of Pandas Dataframe\n",
    "revenue_data_pd_clean = clean_date_columns_pd(dataset=revenue_data_pd, date_columns=date_columns)\n",
    "\n",
    "# clean date column of Spark Dataframe\n",
    "revenue_data_ps_clean = clean_date_columns_ps(dataset=revenue_data_ps, date_columns=date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "963f132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially, the date column in our Pandas DataFrame has dtype given as follows\n",
      "Month    datetime64[ns]\n",
      "dtype: object\n",
      "Here is a small sample of entries:\n",
      "       Month\n",
      "0 2019-04-01\n",
      "1 2019-12-01\n",
      "2 2019-04-01\n",
      "3 2019-12-01\n",
      "4 2019-12-01\n",
      "Initially, the date column in our Pandas DataFrame has datatype given as follows\n",
      "root\n",
      " |-- Month: date (nullable = true)\n",
      "\n",
      "Here is a small sample of entries:\n",
      "+----------+\n",
      "|     Month|\n",
      "+----------+\n",
      "|2019-04-01|\n",
      "|2019-12-01|\n",
      "|2019-04-01|\n",
      "|2019-12-01|\n",
      "|2019-12-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check datatype and content of date column in Pandas DataFrame before applying clean_date_columns_pd \n",
    "print(\"After applying clean_data_columns_pd, the date column in our Pandas DataFrame has dtype given as follows\")\n",
    "print(revenue_data_pd_clean[date_columns].dtypes)\n",
    "print(\"Here is a small sample of entries:\")\n",
    "print(revenue_data_pd_clean[date_columns].head(5))\n",
    "\n",
    "# check datatype and content of date column in PySpark DataFrame before applying clean_date_columns_ps\n",
    "print(\"After applying clean_data_columns_ps, the date column in our PySpark DataFrame has datatype given as follows\")\n",
    "revenue_data_ps_clean.select(*(col(c) for c in date_columns)).printSchema()\n",
    "print(\"Here is a small sample of entries:\")\n",
    "print(revenue_data_ps_clean[date_columns].show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c5dc3",
   "metadata": {},
   "source": [
    "## Translating operations in add_start_end_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8ea74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
